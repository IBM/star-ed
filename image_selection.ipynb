{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "heavy-humor",
   "metadata": {},
   "source": [
    "## Document Convertion, Image Extraction and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-repeat",
   "metadata": {},
   "source": [
    "### Document Convertion\n",
    "To use deepsearch to convert documents to json files to extract images is needed to register [here](https://ds4sd.github.io/deepsearch-toolkit/guide/configuration/) and run:\n",
    "\n",
    "`$ deepsearch profile config\n",
    "Host: https://deepsearch-experience.res.ibm.com\n",
    "Username: name@example.com\n",
    "Api key:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepsearch as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsearch.cps.client.api import CpsApi\n",
    "\n",
    "api = CpsApi.from_env()\n",
    "print([(p.name, p.key) for p in api.projects.list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DOCS = \"PATH OR LINK TO PDF.pdf\"\"\n",
    "PROJ_KEY = \"\"\n",
    "RESULT_DIR = \"PATH TO OUTPUT FOLDER\"\n",
    "\n",
    "# for online documents use urls= and for local files use source_path\n",
    "documents = ds.convert_documents(api=api, proj_key=PROJ_KEY, urls=PATH_DOCS)\n",
    "\n",
    "documents.download_all(result_dir=RESULT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-poster",
   "metadata": {},
   "source": [
    "this cell will return a json file with information regarding the extraction output of the submissted pdf, the \"figures\" section is the one that we will use to extract the images that we're interested in with bbox information\n",
    "\n",
    "`{\"_name\": \"s41746-023-00881-0.pdf\", \"_type\": \"pdf-document\", ...,  \"figures\": [{\"bounding-box\": {\"max\": [6.9888954, 241.64507, 595.276, 734.6084], \"min\": [47.72477340698242, 243.7888793945312, 555.028564453125, 733.34814453125]}, \"cells\": {\"data\": [], \"header\": [\"x0\", \"y0\", \"x1\", \"y1\", \"font\", \"text\"]}, \"confidence\": 0.865432858467102, \"created_by\": \"high_conf_pred\", \"prov\": [{\"bbox\": [47.72477340698242, 243.7888793945312, 555.028564453125, 733.34814453125], \"page\": 2, \"span\": [0, 0]}], ....`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-jonathan",
   "metadata": {},
   "source": [
    "### Image Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "neutral-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "\n",
    "class PdfFileWriterWithStreamAttribute(PdfWriter):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        from io import BytesIO\n",
    "\n",
    "        self.stream = BytesIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "initial-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "JSON_FILE = \"PATH TO JSON.json\"\n",
    "PDF_FILE = \"PATH TO PDF.pdf\"\n",
    "CROP_DIR = \"PATH TO SAVE CROPPED IMAGES FOLDER\"\n",
    "\n",
    "jsonFile = open(JSON_FILE)\n",
    "jsonData = json.load(jsonFile)\n",
    "jsonFile.close()\n",
    "figures = jsonData[\"figures\"]  # use directories for multiple pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-money",
   "metadata": {},
   "source": [
    "Looks for bounding boxes and exports individual files per each figure found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(PDF_FILE, \"r\")\n",
    "if os.path.exists(JSON_FILE):\n",
    "    with open(JSON_FILE) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    for image_num in range(len(figures) - 1):\n",
    "        cors = data[\"figures\"][image_num][\"prov\"][0][\"bbox\"]\n",
    "        page_num = data[\"figures\"][image_num][\"prov\"][0][\"page\"] - 1\n",
    "        page = reader.pages[page_num]\n",
    "        writer = PdfFileWriterWithStreamAttribute()\n",
    "        page.cropbox.upper_right = (cors[2], cors[3])\n",
    "        page.cropbox.lower_left = (cors[0], cors[1])\n",
    "        writer.add_page(page)\n",
    "        outstream = open(\n",
    "            os.path.join(\n",
    "                CROP_DIR,\n",
    "                os.path.split(PDF_FILE)[-1].split(\".pdf\")[0]\n",
    "                + \"_cropped_page_\"\n",
    "                + str(page_num + 1)\n",
    "                + \".pdf\",\n",
    "            ),\n",
    "            \"wb\",\n",
    "        )\n",
    "        writer.write(outstream)\n",
    "        outstream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-judgment",
   "metadata": {},
   "source": [
    "### Image Selection\n",
    "\n",
    "Note that for training this model you can request **DermEducation**. This image set of dermatology images used for educational purposes. **DermEducation** contains containing 2708 total images, among which 461 are non-skin images, 2247 skin images (1932 FST I-IV and 315 FST V-VI).\n",
    "\n",
    "- $X$ will contain the feature that wants to be used Histogram of Oriented Gradient (HoG) and mean and standard deviations of image channels in CIELAB (24) color space.\n",
    "\n",
    "- $y$ will contain binary labels, skin and non-skin images.\n",
    "\n",
    "#### 1. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing_utils import preprocesing_masks_for_classification\n",
    "\n",
    "_, _, features_skin = preprocesing_masks_for_classification(images_w_skin, ita=False)\n",
    "_, _, features_non_skin = preprocesing_masks_for_classification(\n",
    "    images_w_skin, ita=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-prayer",
   "metadata": {},
   "source": [
    "#### 2. Train Binary XGBoost with feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit(X).transform(X)\n",
    "\n",
    "data_dmatrix = xgb.DMatrix(data=X_scaled, label=y)\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"eta\": 0.03,\n",
    "    \"subsample\": 0.5,\n",
    "}\n",
    "xgb_cv = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3, metrics=\"error\", seed=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
